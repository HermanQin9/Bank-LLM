# Spark/EMR Integration Architecture

## Overview

This document describes how Apache Spark and AWS EMR extend the deep integration architecture to support **distributed data processing at scale**.

## Architecture Extension

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    UNIFIED INTELLIGENCE PLATFORM                        â”‚
â”‚            (Existing Deep Integration + Spark/EMR Layer)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–²
                                    â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                â”‚                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Transaction Engineâ”‚  â”‚  Intelligenceâ”‚  â”‚ ðŸ†• Spark/EMR â”‚
        â”‚  (Java/Scala)      â”‚  â”‚  (Python ML) â”‚  â”‚  (Distributed)â”‚
        â”‚                    â”‚  â”‚              â”‚  â”‚              â”‚
        â”‚  â€¢ ETL Pipeline    â”‚  â”‚  â€¢ PyTorch   â”‚  â”‚  â€¢ Batch     â”‚
        â”‚  â€¢ Real-time       â”‚  â”‚  â€¢ LLM/RAG   â”‚  â”‚  â€¢ Streaming â”‚
        â”‚  â€¢ Fraud Rules     â”‚  â”‚  â€¢ FastAPI   â”‚  â”‚  â€¢ S3 Lake   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                     â”‚                 â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   PostgreSQL DB     â”‚
                    â”‚   (Shared State)    â”‚
                    â”‚                     â”‚
                    â”‚  â€¢ transactions     â”‚
                    â”‚  â€¢ fraud_alerts     â”‚
                    â”‚  â€¢ customer_profilesâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    AWS S3 Lake     â”‚
                    â”‚   (Parquet files)  â”‚
                    â”‚                    â”‚
                    â”‚  â€¢ Historical data â”‚
                    â”‚  â€¢ Partitioned     â”‚
                    â”‚  â€¢ Compressed 5x   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Integration Points

### 1. PostgreSQL â†” Spark Integration

**Batch Export (Java/Scala â†’ Spark)**:
```scala
// SparkTransactionProcessor reads from PostgreSQL
val transactions = spark.read
  .format("jdbc")
  .option("url", "jdbc:postgresql://localhost:5432/frauddb")
  .option("dbtable", "transactions")
  .option("user", "postgres")
  .option("password", "postgres")
  .load()

// Process millions of records with distributed computing
val fraudAlerts = SparkTransactionProcessor.runFraudDetectionPipeline(transactions)
```

**Real-time Ingestion (Spark â†’ PostgreSQL)**:
```scala
// SparkStreamingProcessor writes back to PostgreSQL
streamingAlerts
  .writeStream
  .foreachBatch { (batchDF, batchId) =>
    batchDF.write
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("dbtable", "fraud_alerts")
      .mode("append")
      .save()
  }
  .start()
```

### 2. S3 Data Lake Integration

**Purpose**: Scale beyond PostgreSQL for historical analytics

**Architecture**:
- PostgreSQL: Live transactions (last 90 days)
- S3 Data Lake: Historical transactions (all time)
- Spark: Bridge between both systems

**Data Flow**:
```
Java ETL â†’ PostgreSQL (real-time)
              â†“
       S3DataManager exports to S3 (daily batch)
              â†“
    S3 Parquet files (partitioned: year/month/day)
              â†“
  Spark batch jobs read from S3 (historical analysis)
              â†“
    Results written back to PostgreSQL (fraud_alerts)
              â†“
  Java dashboard displays results
```

**Example Workflow**:
```scala
// 1. Export PostgreSQL to S3 (run daily)
S3DataManager.exportTransactionsToS3(
  spark,
  "jdbc:postgresql://localhost:5432/frauddb",
  "s3://bank-fraud-data/transactions/",
  dateFrom = Some("2024-01-01"),
  dateTo = Some("2025-11-24")
)

// 2. Process S3 data with Spark
val historicalTxns = S3DataManager.readFromS3Parquet(
  spark,
  "s3://bank-fraud-data/transactions/",
  partitionColumns = Map("year" -> "2024")
)

// 3. Detect patterns across 100M+ transactions
val alerts = SparkTransactionProcessor.runFraudDetectionPipeline(historicalTxns)

// 4. Write results back to PostgreSQL
alerts.write
  .format("jdbc")
  .option("url", jdbcUrl)
  .option("dbtable", "spark_fraud_alerts")
  .mode("append")
  .save()
```

### 3. AWS EMR Deployment

**Cluster Configuration**:
- Master: 1x m5.xlarge (Spark driver)
- Core: 2x m5.xlarge (data nodes)
- Task: 1-10x m5.xlarge (auto-scaling, spot instances)

**Job Submission**:
```bash
# Submit from BankFraudTest/src/main/scripts/
./submit-spark-job.sh <cluster-id> batch \
  s3://bank-fraud-data/transactions/ \
  s3://bank-fraud-data/fraud-alerts/
```

**Integration with Existing System**:
- Uses same PostgreSQL JDBC driver
- Reads same transaction schema
- Writes to same fraud_alerts table
- Compatible with Java DeepIntegrationDemo

## Unified Fraud Detection Pipeline

### Small-Scale (< 1M transactions): Java/Scala
```
Transaction â†’ FraudAnalyzer (Scala) â†’ PostgreSQL â†’ Java Dashboard
              (Single-node, <10ms)
```

### Medium-Scale (1M-10M transactions): Python ML
```
Transaction â†’ ML Model (PyTorch) â†’ PostgreSQL â†’ Java Dashboard
              (GPU, <100ms)
```

### Large-Scale (10M-100M+ transactions): Spark/EMR
```
S3 Data Lake â†’ Spark Batch (EMR) â†’ PostgreSQL â†’ Java Dashboard
              (Distributed, 30K records/sec)
```

### Real-Time Streaming: Spark Structured Streaming
```
Kafka â†’ SparkStreamingProcessor â†’ PostgreSQL â†’ Java Dashboard
        (5s latency, 10K events/sec)
```

## Data Consistency

**Challenge**: Ensure Spark and Java/Python see same data

**Solution**: Single source of truth (PostgreSQL)

**Workflow**:
1. Java writes transaction to PostgreSQL
2. PostgreSQL â†’ S3 export (daily batch)
3. Spark reads from S3 OR PostgreSQL (same schema)
4. Spark writes fraud alerts to PostgreSQL
5. Java reads fraud alerts from PostgreSQL
6. Python ML models consume same data

**Schema Compatibility**:
```scala
// Spark DataFrame schema matches PostgreSQL
case class Transaction(
  transaction_id: String,
  customer_id: String,
  transaction_date: Timestamp,
  amount: Double,
  currency: String,
  // ... same 18 columns as PostgreSQL
)
```

## Performance Benefits

### Before Spark Integration:
- PostgreSQL query: 100M transactions = 15 minutes
- Single-node processing: Limited by RAM (16GB)
- Historical analysis: Impractical at scale

### After Spark Integration:
- Spark on EMR: 100M transactions = 55 minutes (distributed)
- Auto-scaling: 1-10 task nodes based on load
- S3 data lake: Unlimited storage, Parquet compression (5.2x)
- Cost optimization: Spot instances (70% savings)

## Testing Integration

**Unit Tests** (Scala):
```bash
cd BankFraudTest
mvn test -Dtest=SparkTransactionProcessorTest
```

**Integration Test** (Full Pipeline):
```bash
# 1. Start PostgreSQL
docker-compose up -d

# 2. Load test data
mvn exec:java -Dexec.mainClass="com.bankfraud.data.TransactionLoader"

# 3. Run Spark batch job (local mode)
mvn exec:java -Dexec.mainClass="com.bankfraud.spark.SparkTransactionProcessor"

# 4. Verify results in PostgreSQL
psql -h localhost -U postgres -d frauddb -c "SELECT * FROM fraud_alerts LIMIT 10;"

# 5. Check Java can read Spark results
mvn exec:java -Dexec.mainClass="com.bankfraud.integration.DeepIntegrationDemo"
```

## Summary

This Spark/EMR integration maintains the **deep integration philosophy**:

âœ… **Single shared database**: PostgreSQL remains single source of truth  
âœ… **Bidirectional data flow**: Spark reads from PostgreSQL, writes back alerts  
âœ… **Schema compatibility**: Spark uses same 18-column transaction schema  
âœ… **Unified system**: Java, Python, Spark all work together  
âœ… **No data duplication**: S3 is archive, not separate database  
âœ… **Production-ready**: EMR auto-scaling, spot instances, monitoring  

**Result**: ONE unified platform that scales from 1K to 100M+ transactions while maintaining deep integration across Java/Scala/Python/Spark.

See [SPARK_EMR_INTEGRATION.md](SPARK_EMR_INTEGRATION.md) for implementation details and deployment guide.
