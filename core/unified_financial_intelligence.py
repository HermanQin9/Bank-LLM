"""
Unified Financial Intelligence System - Core Integration Module

This is the REAL integration layer that deeply connects:
1. Transaction data (PostgreSQL - 2.2M+ records)
2. Java/Scala fraud detection (rule engine + statistics)
3. Document intelligence (LLM + RAG)
4. ML models (PyTorch embeddings + classification)

Real-world scenarios implemented:
- Automatic customer profile extraction from onboarding documents
- Transaction-document cross-validation for compliance
- Real-time risk assessment combining structured + unstructured data
- Automated regulatory report generation (SAR, CTR)

Author: Herman Qin
Date: November 2025
"""

import os
import sys
import asyncio
import json
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import psycopg2
from psycopg2.extras import RealDictCursor, Json
from psycopg2.extensions import connection as PGConnection
import numpy as np

# Add paths
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(project_root, 'LLM', 'src'))

# LLM and RAG imports
from llm_engine.universal_client import UniversalLLMClient
from rag_system.gemini_rag_pipeline import GeminiRAGPipeline

# Logger
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class CustomerProfile:
    """Customer profile extracted from onboarding documents using LLM."""
    customer_id: str
    business_type: str  # e.g., "E-commerce", "Import/Export", "Consulting"
    expected_monthly_volume: float
    expected_transaction_size: Tuple[float, float]  # (min, max)
    geographic_scope: List[str]  # ["Domestic", "International - Asia", etc.]
    risk_indicators: List[str]
    kyc_document_source: str
    extracted_at: str
    confidence_score: float


@dataclass
class TransactionAlert:
    """Alert generated when transaction deviates from customer profile."""
    alert_id: str
    transaction_id: str
    customer_id: str
    alert_type: str  # "AMOUNT_ANOMALY", "GEO_MISMATCH", "FREQUENCY_SPIKE", etc.
    severity: str  # "LOW", "MEDIUM", "HIGH", "CRITICAL"
    deviation_details: Dict[str, Any]
    supporting_evidence: List[str]  # Document excerpts
    recommended_action: str
    created_at: str


class DatabaseConnector:
    """
    Manages connection to PostgreSQL database containing transaction data.
    Uses connection pooling for production performance.
    """
    
    def __init__(self, connection_string: Optional[str] = None):
        self.connection_string = connection_string or os.getenv(
            'DATABASE_URL',
            'postgresql://postgres:postgres123@localhost:5432/frauddb'
        )
        self.conn: Optional[PGConnection] = None
        logger.info("Database connector initialized")
    
    def connect(self) -> PGConnection:
        """Establish database connection and return connection handle."""
        if not self.conn or self.conn.closed:
            self.conn = psycopg2.connect(self.connection_string)
            logger.info(" Connected to PostgreSQL database")
        return self.conn
    
    def disconnect(self):
        """Close database connection."""
        if self.conn and not self.conn.closed:
            self.conn.close()
            logger.info("Disconnected from database")
    
    def get_customer_transactions(self, customer_id: str, days: int = 30) -> List[Dict]:
        """Retrieve recent transactions for a customer."""
        conn = self.connect()
        query = """
            SELECT 
                transaction_id,
                customer_id,
                amount,
                merchant_name,
                merchant_category,
                transaction_date,
                location,
                raw_data
            FROM transactions
            WHERE customer_id = %s
                AND transaction_date >= NOW() - INTERVAL '%s days'
            ORDER BY transaction_date DESC
        """
        
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute(query, (customer_id, days))
            results = cursor.fetchall()
            logger.info(f" Retrieved {len(results)} transactions for customer {customer_id}")
            return [dict(row) for row in results]
    
    def get_customer_statistics(self, customer_id: str) -> Dict:
        """Calculate customer transaction statistics (mimics Scala TransactionStatistics)."""
        conn = self.connect()
        query = """
            SELECT 
                COUNT(*) as total_transactions,
                AVG(amount) as avg_amount,
                STDDEV(amount) as std_amount,
                MIN(amount) as min_amount,
                MAX(amount) as max_amount,
                PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY amount) as median_amount,
                PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY amount) as p95_amount,
                COUNT(DISTINCT merchant_name) as unique_merchants,
                COUNT(DISTINCT location) as unique_locations
            FROM transactions
            WHERE customer_id = %s
        """
        
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute(query, (customer_id,))
            result = cursor.fetchone()
            return dict(result) if result else {}

    def get_customer_profile(self, customer_id: str) -> Optional[CustomerProfile]:
        """Fetch previously extracted customer profile for shared feature store."""
        conn = self.connect()
        query = """
            SELECT 
                customer_id,
                business_type,
                expected_monthly_volume,
                expected_min_amount,
                expected_max_amount,
                geographic_scope,
                risk_indicators,
                kyc_document_source,
                confidence_score,
                COALESCE(updated_at, created_at) AS profile_timestamp
            FROM customer_profiles
            WHERE customer_id = %s
        """

        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute(query, (customer_id,))
            row = cursor.fetchone()
            if not row:
                return None

            min_amount = float(row.get('expected_min_amount') or 0.0)
            max_amount = float(row.get('expected_max_amount') or 0.0)
            geo_scope = row.get('geographic_scope') or []
            risk_indicators = row.get('risk_indicators') or []
            timestamp = row.get('profile_timestamp')

            return CustomerProfile(
                customer_id=row['customer_id'],
                business_type=row.get('business_type', 'Unknown'),
                expected_monthly_volume=float(row.get('expected_monthly_volume') or 0.0),
                expected_transaction_size=(min_amount, max_amount),
                geographic_scope=geo_scope,
                risk_indicators=risk_indicators,
                kyc_document_source=row.get('kyc_document_source', ''),
                extracted_at=timestamp.isoformat() if timestamp else datetime.now().isoformat(),
                confidence_score=float(row.get('confidence_score') or 0.0)
            )
    
    def save_customer_profile(self, profile: CustomerProfile):
        """Save extracted customer profile to database."""
        conn = self.connect()
        query = """
            INSERT INTO customer_profiles (
                customer_id, business_type, expected_monthly_volume,
                expected_min_amount, expected_max_amount, geographic_scope,
                risk_indicators, kyc_document_source, confidence_score, created_at
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (customer_id) DO UPDATE SET
                business_type = EXCLUDED.business_type,
                expected_monthly_volume = EXCLUDED.expected_monthly_volume,
                updated_at = NOW()
        """
        
        with conn.cursor() as cursor:
            cursor.execute(query, (
                profile.customer_id,
                profile.business_type,
                profile.expected_monthly_volume,
                profile.expected_transaction_size[0],
                profile.expected_transaction_size[1],
                json.dumps(profile.geographic_scope),
                json.dumps(profile.risk_indicators),
                profile.kyc_document_source,
                profile.confidence_score,
                profile.extracted_at
            ))
            conn.commit()
            logger.info(f" Saved customer profile for {profile.customer_id}")

    def save_transaction_alert(self, alert: TransactionAlert):
        """Persist unified transaction alerts for downstream Java services."""
        conn = self.connect()
        query = """
            INSERT INTO transaction_alerts (
                alert_id,
                transaction_id,
                customer_id,
                alert_type,
                severity,
                deviation_details,
                supporting_evidence,
                recommended_action,
                status
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (alert_id) DO UPDATE SET
                severity = EXCLUDED.severity,
                deviation_details = EXCLUDED.deviation_details,
                supporting_evidence = EXCLUDED.supporting_evidence,
                recommended_action = EXCLUDED.recommended_action,
                status = EXCLUDED.status,
                resolved_at = CASE WHEN EXCLUDED.status = 'CLOSED' THEN NOW() ELSE transaction_alerts.resolved_at END
        """

        with conn.cursor() as cursor:
            cursor.execute(query, (
                alert.alert_id,
                alert.transaction_id,
                alert.customer_id,
                alert.alert_type,
                alert.severity,
                Json(alert.deviation_details or {}),
                Json(alert.supporting_evidence or []),
                alert.recommended_action,
                'PENDING'
            ))
            conn.commit()
            logger.info(f" Persisted transaction alert {alert.alert_id} -> transaction_alerts")

    def save_document_evidence(self, alert_id: str, customer_id: str, transaction_id: str, evidence_list: List[str]):
        """Persist supporting evidence snippets to document_evidence table."""
        if not evidence_list:
            return

        conn = self.connect()
        insert_query = """
            INSERT INTO document_evidence (
                alert_id,
                transaction_id,
                customer_id,
                document_type,
                excerpt,
                relevance_score
            ) VALUES (%s, %s, %s, %s, %s, %s)
        """

        def _parse_score(snippet: str) -> Optional[float]:
            if not snippet.startswith("[Score:"):
                return None
            try:
                prefix = snippet.split(']')[0]
                score_text = prefix.split(':')[1].strip()
                return float(score_text)
            except (ValueError, IndexError):
                return None

        with conn.cursor() as cursor:
            for excerpt in evidence_list:
                cursor.execute(
                    insert_query,
                    (
                        alert_id,
                        transaction_id,
                        customer_id,
                        'RAG_SNIPPET',
                        excerpt,
                        _parse_score(excerpt)
                    )
                )
            conn.commit()
            logger.info(f" Stored {len(evidence_list)} evidence snippets for alert {alert_id}")


class DocumentIntelligenceEngine:
    """
    Uses LLM + RAG to extract intelligence from financial documents.
    Deeply integrated with transaction data for cross-validation.
    """
    
    def __init__(self):
        self.llm = UniversalLLMClient()
        self.rag = GeminiRAGPipeline(llm_provider="gemini")
        logger.info(" Document intelligence engine initialized")
    
    async def extract_customer_profile_from_kyc(self, document_path: str, customer_id: str) -> CustomerProfile:
        """
        Extract customer business profile from KYC documents.
        This is REAL integration: documents ‚Üí structured data ‚Üí database ‚Üí transaction validation
        """
        logger.info(f" Extracting customer profile from: {document_path}")
        
        # Read document
        try:
            with open(document_path, 'r', encoding='utf-8') as f:
                document_text = f.read()
        except Exception as e:
            logger.error(f"Failed to read document: {e}")
            # In production: use PDF parser
            document_text = "Sample KYC document text"
        
        # LLM extraction prompt
        prompt = f"""
Analyze this customer KYC (Know Your Customer) document and extract key business information:

DOCUMENT:
{document_text[:3000]}  # Truncate to fit context

Extract and provide in JSON format:
1. business_type: What type of business (e.g., "Retail", "E-commerce", "Import/Export", "Consulting")
2. expected_monthly_volume: Estimated monthly transaction volume in USD
3. expected_transaction_size: [min_amount, max_amount] typical transaction range
4. geographic_scope: List of countries/regions for business operations
5. risk_indicators: Any red flags mentioned (PEP status, high-risk jurisdiction, etc.)

Return ONLY valid JSON, no explanation:
{{
    "business_type": "...",
    "expected_monthly_volume": 50000.0,
    "expected_transaction_size": [100.0, 5000.0],
    "geographic_scope": ["USA", "Canada"],
    "risk_indicators": ["None"] or ["PEP", "High-risk jurisdiction"]
}}
"""
        
        try:
            response = self.llm.generate(prompt, temperature=0.1, max_tokens=500)
            
            # Parse JSON response
            # Handle potential markdown code blocks
            if "```json" in response:
                response = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                response = response.split("```")[1].split("```")[0].strip()
            
            extracted_data = json.loads(response)
            
            # Create CustomerProfile
            profile = CustomerProfile(
                customer_id=customer_id,
                business_type=extracted_data.get('business_type', 'Unknown'),
                expected_monthly_volume=float(extracted_data.get('expected_monthly_volume', 0)),
                expected_transaction_size=tuple(extracted_data.get('expected_transaction_size', [0, 0])),
                geographic_scope=extracted_data.get('geographic_scope', []),
                risk_indicators=extracted_data.get('risk_indicators', []),
                kyc_document_source=document_path,
                extracted_at=datetime.now().isoformat(),
                confidence_score=0.85  # In production: calculate from LLM response quality
            )
            
            logger.info(f" Extracted profile: {profile.business_type}, expected ${profile.expected_monthly_volume}/month")
            return profile
            
        except Exception as e:
            logger.error(f" Profile extraction failed: {e}")
            # Return default profile
            return CustomerProfile(
                customer_id=customer_id,
                business_type="Unknown",
                expected_monthly_volume=0.0,
                expected_transaction_size=(0.0, 0.0),
                geographic_scope=[],
                risk_indicators=[],
                kyc_document_source=document_path,
                extracted_at=datetime.now().isoformat(),
                confidence_score=0.0
            )
    
    async def find_supporting_evidence(self, customer_id: str, alert_type: str, query: str) -> List[str]:
        """
        Search document store for evidence related to an alert.
        REAL integration: transaction alert ‚Üí document search ‚Üí contextual evidence
        """
        logger.info(f" Searching documents for evidence: {alert_type}")
        
        try:
            # Semantic search in RAG system
            search_query = f"customer {customer_id} {alert_type} {query}"
            results = self.rag.semantic_search_only(search_query, top_k=3)
            
            evidence = []
            for result in results:
                excerpt = result['document'][:300]  # First 300 chars
                evidence.append(f"[Score: {result['score']:.3f}] {excerpt}")
            
            logger.info(f"üìé Found {len(evidence)} supporting documents")
            return evidence
            
        except Exception as e:
            logger.warning(f"  Document search failed: {e}")
            return ["No supporting documents found"]
    
    async def generate_sar_report(self, customer_id: str, transactions: List[Dict], 
                                   alerts: List[TransactionAlert]) -> str:
        """
        Generate Suspicious Activity Report (SAR) combining transaction data + document evidence.
        THIS IS THE REAL FUSION: structured data + unstructured insights + LLM reasoning
        """
        logger.info(f"üìù Generating SAR report for customer {customer_id}")
        
        # Prepare transaction summary
        total_amount = sum(tx['amount'] for tx in transactions)
        tx_summary = f"""
Total Suspicious Transactions: {len(transactions)}
Total Amount: ${total_amount:,.2f}
Date Range: {transactions[-1]['transaction_date']} to {transactions[0]['transaction_date']}
"""
        
        # Prepare alert summary
        alert_summary = "\n".join([
            f"- {alert.alert_type}: {alert.severity} - {alert.recommended_action}"
            for alert in alerts
        ])
        
        # Get supporting evidence from documents
        evidence = []
        for alert in alerts[:3]:  # Top 3 alerts
            docs = await self.find_supporting_evidence(
                customer_id, 
                alert.alert_type,
                f"{alert.deviation_details}"
            )
            evidence.extend(docs)
        
        # LLM generates professional SAR report
        prompt = f"""
Generate a professional Suspicious Activity Report (SAR) for regulatory submission:

CUSTOMER ID: {customer_id}

TRANSACTION SUMMARY:
{tx_summary}

ALERTS TRIGGERED:
{alert_summary}

SUPPORTING EVIDENCE FROM CUSTOMER FILES:
{chr(10).join(evidence[:5])}

Generate a formal SAR report including:
1. Executive Summary
2. Suspicious Activity Description
3. Transaction Pattern Analysis
4. Supporting Documentation References
5. Recommended Regulatory Actions

Format as professional regulatory document.
"""
        
        report = self.llm.generate(prompt, temperature=0.3, max_tokens=2000)
        logger.info(f" Generated {len(report)} character SAR report")
        return report


class UnifiedFinancialIntelligence:
    """
    MAIN INTEGRATION CLASS - This is where the magic happens.
    
    Combines:
    1. Transaction data (PostgreSQL)
    2. Customer profiles (extracted from documents by LLM)
    3. Real-time monitoring (compare transactions vs. profile)
    4. Evidence gathering (RAG system)
    5. Regulatory reporting (LLM synthesis)
    """
    
    def __init__(self, db_connection_string: Optional[str] = None):
        self.db = DatabaseConnector(db_connection_string)
        self.doc_engine = DocumentIntelligenceEngine()
        logger.info(" Unified Financial Intelligence System initialized")
    
    async def onboard_customer(self, customer_id: str, kyc_document_path: str) -> CustomerProfile:
        """
        Customer onboarding workflow - REAL FUSION EXAMPLE 1
        
        Flow:
        1. LLM extracts business profile from KYC documents
        2. Save profile to PostgreSQL
        3. Use profile for ongoing transaction monitoring
        """
        logger.info(f"\n{'='*70}")
        logger.info(f"üÜï CUSTOMER ONBOARDING: {customer_id}")
        logger.info(f"{'='*70}\n")
        
        # Extract profile from document
        profile = await self.doc_engine.extract_customer_profile_from_kyc(
            kyc_document_path, 
            customer_id
        )
        
        # Save to database
        self.db.save_customer_profile(profile)
        
        logger.info(f" Customer {customer_id} onboarded successfully")
        logger.info(f"   Business: {profile.business_type}")
        logger.info(f"   Expected volume: ${profile.expected_monthly_volume:,.0f}/month")
        logger.info(f"   Transaction range: ${profile.expected_transaction_size[0]:,.0f} - ${profile.expected_transaction_size[1]:,.0f}")
        
        return profile
    
    async def monitor_transaction(self, transaction: Dict) -> Optional[TransactionAlert]:
        """
        Real-time transaction monitoring - REAL FUSION EXAMPLE 2
        
        Flow:
        1. Get customer profile (from LLM-extracted data)
        2. Get transaction statistics (from PostgreSQL + Scala logic)
        3. Compare transaction against profile
        4. If anomaly: search documents for context
        5. Generate alert with evidence
        """
        customer_id = transaction['customer_id']
        amount = transaction['amount']
        
        logger.info(f"\n{'='*70}")
        logger.info(f" MONITORING TRANSACTION: {transaction['transaction_id']}")
        logger.info(f"{'='*70}\n")
        
        # Shared feature store lookup
        stats = self.db.get_customer_statistics(customer_id)
        profile = self.db.get_customer_profile(customer_id)
        if not profile:
            logger.info("  No stored profile found for customer ‚Äî falling back to statistics only")
        
        alert_candidate: Optional[TransactionAlert] = None
        
        if profile:
            alert_candidate = self._evaluate_profile_deviation(transaction, profile)
            if alert_candidate:
                logger.warning(f"  PROFILE deviation detected for {customer_id}")
        
        # Amount anomaly (statistical deviation)
        avg_amount = stats.get('avg_amount', 0) if stats else 0
        std_amount = stats.get('std_amount', 0) if stats else 0
        z_score = abs((amount - avg_amount) / std_amount) if std_amount and std_amount > 0 else 0
        
        if not alert_candidate and z_score > 3:
            alert_candidate = TransactionAlert(
                alert_id=f"ALERT-{datetime.now().strftime('%Y%m%d%H%M%S')}",
                transaction_id=transaction['transaction_id'],
                customer_id=customer_id,
                alert_type="AMOUNT_ANOMALY",
                severity="HIGH" if z_score > 4 else "MEDIUM",
                deviation_details={
                    'transaction_amount': amount,
                    'customer_avg': avg_amount,
                    'z_score': z_score,
                    'typical_range': f"${avg_amount - 2*std_amount:.2f} - ${avg_amount + 2*std_amount:.2f}"
                },
                supporting_evidence=[],
                recommended_action="MANUAL_REVIEW" if z_score > 4 else "FLAG_FOR_MONITORING",
                created_at=datetime.now().isoformat()
            )
            logger.warning(f"  Statistical ALERT triggered (z-score={z_score:.2f})")
        
        if alert_candidate:
            evidence = await self.doc_engine.find_supporting_evidence(
                customer_id,
                alert_candidate.alert_type,
                json.dumps(alert_candidate.deviation_details)
            )
            alert_candidate.supporting_evidence = evidence
            
            self.db.save_transaction_alert(alert_candidate)
            self.db.save_document_evidence(
                alert_candidate.alert_id,
                alert_candidate.customer_id,
                alert_candidate.transaction_id,
                evidence
            )
            
            logger.info(f"   Persisted alert {alert_candidate.alert_id} with {len(evidence)} evidence snippets")
            return alert_candidate
        
        logger.info(f" Transaction approved - within normal range")
        logger.info(f"   Amount: ${amount:,.2f} (Z-score: {z_score:.2f})")
        return None

    def _evaluate_profile_deviation(self, transaction: Dict, profile: CustomerProfile) -> Optional[TransactionAlert]:
        """Compare transaction attributes against stored profile to flag mismatches."""
        amount = transaction['amount']
        min_expected, max_expected = profile.expected_transaction_size
        deviations: Dict[str, Any] = {
            'profile_business_type': profile.business_type,
            'profile_confidence': profile.confidence_score
        }
        severity: Optional[str] = None
        
        if max_expected and max_expected > 0 and amount > max_expected * 1.5:
            deviations['amount_vs_profile'] = {
                'amount': amount,
                'expected_max': max_expected,
                'breach_factor': round(amount / max_expected, 2)
            }
            severity = 'CRITICAL' if amount > max_expected * 2 else 'HIGH'
        elif min_expected and min_expected > 0 and amount < max(1.0, min_expected * 0.5):
            deviations['amount_vs_profile'] = {
                'amount': amount,
                'expected_min': min_expected
            }
            severity = 'MEDIUM'
        
        location = transaction.get('location')
        if location and profile.geographic_scope:
            normalized_scope = [scope.lower() for scope in profile.geographic_scope]
            loc_lower = location.lower()
            if loc_lower not in normalized_scope and not any(loc_lower in scope for scope in normalized_scope):
                deviations['geography_mismatch'] = {
                    'transaction_location': location,
                    'expected_scope': profile.geographic_scope
                }
                severity = severity or 'MEDIUM'
        
        if len(deviations.keys()) <= 2:  # Only metadata populated
            return None
        
        recommended_action = 'BLOCK_TRANSACTION' if severity == 'CRITICAL' else 'MANUAL_REVIEW'
        if severity == 'MEDIUM':
            recommended_action = 'FLAG_FOR_MONITORING'
        
        return TransactionAlert(
            alert_id=f"ALERT-{datetime.now().strftime('%Y%m%d%H%M%S')}",
            transaction_id=transaction['transaction_id'],
            customer_id=transaction['customer_id'],
            alert_type="PROFILE_DEVIATION",
            severity=severity or 'MEDIUM',
            deviation_details=deviations,
            supporting_evidence=[],
            recommended_action=recommended_action,
            created_at=datetime.now().isoformat()
        )
    
    async def generate_compliance_report(self, customer_id: str, days: int = 30) -> str:
        """
        Automated compliance reporting - REAL FUSION EXAMPLE 3
        
        Flow:
        1. Query suspicious transactions from PostgreSQL
        2. Aggregate alerts
        3. Search documents for regulatory context
        4. LLM generates professional report combining all sources
        """
        logger.info(f"\n{'='*70}")
        logger.info(f" GENERATING COMPLIANCE REPORT: {customer_id}")
        logger.info(f"{'='*70}\n")
        
        # Get transactions
        transactions = self.db.get_customer_transactions(customer_id, days)
        
        if not transactions:
            return f"No transactions found for customer {customer_id} in last {days} days."
        
        # Analyze each transaction
        alerts = []
        for tx in transactions:
            alert = await self.monitor_transaction(tx)
            if alert:
                alerts.append(alert)
        
        if not alerts:
            logger.info(f" No suspicious activity detected")
            return f"Customer {customer_id}: No suspicious activity in last {days} days."
        
        # Generate SAR report
        report = await self.doc_engine.generate_sar_report(customer_id, transactions, alerts)
        
        logger.info(f" Compliance report generated")
        logger.info(f"   Suspicious transactions: {len([a for a in alerts if a.severity in ['HIGH', 'CRITICAL']])}")
        logger.info(f"   Total alerts: {len(alerts)}")
        
        return report


# Example usage demonstrating REAL integration
async def main():
    """
    Demonstration of deep integration between transaction processing and document intelligence.
    """
    
    # Initialize system
    system = UnifiedFinancialIntelligence()
    
    print("\n" + "="*70)
    print("üè¶ UNIFIED FINANCIAL INTELLIGENCE SYSTEM - DEMO")
    print("="*70)
    
    # Scenario 1: Customer Onboarding
    print("\nüìã SCENARIO 1: Customer Onboarding with Document Analysis")
    print("-" * 70)
    
    # Simulate KYC document
    kyc_doc_path = "sample_kyc.txt"
    with open(kyc_doc_path, 'w') as f:
        f.write("""
        CUSTOMER DUE DILIGENCE FORM
        
        Company Name: TechCorp International Ltd.
        Business Type: E-commerce platform for electronics
        Expected Monthly Volume: Approximately $150,000 - $200,000
        Typical Transaction Size: $500 - $8,000 per order
        Geographic Scope: Primarily USA and Canada, occasional shipments to Europe
        
        Risk Assessment: Low risk. Established business with 5-year track record.
        No PEP (Politically Exposed Person) connections identified.
        """)
    
    profile = await system.onboard_customer("CUST-12345", kyc_doc_path)
    
    # Scenario 2: Real-time Transaction Monitoring
    print("\n\n SCENARIO 2: Real-time Transaction Monitoring")
    print("-" * 70)
    
    # Simulate normal transaction
    normal_tx = {
        'transaction_id': 'TXN-001',
        'customer_id': 'CUST-12345',
        'amount': 2500.00,
        'merchant_name': 'Electronics Wholesale Inc',
        'merchant_category': 'Retail',
        'transaction_date': datetime.now().isoformat(),
        'location': 'USA'
    }
    
    print("\n Testing normal transaction...")
    alert1 = await system.monitor_transaction(normal_tx)
    
    # Simulate anomalous transaction
    anomalous_tx = {
        'transaction_id': 'TXN-002',
        'customer_id': 'CUST-12345',
        'amount': 75000.00,  # Way above expected range
        'merchant_name': 'Unknown Offshore Vendor',
        'merchant_category': 'Wire Transfer',
        'transaction_date': datetime.now().isoformat(),
        'location': 'Cayman Islands'
    }
    
    print("\n Testing anomalous transaction...")
    alert2 = await system.monitor_transaction(anomalous_tx)
    
    if alert2:
        print("\n  ALERT DETAILS:")
        print(json.dumps(asdict(alert2), indent=2, default=str))
    
    # Scenario 3: Compliance Reporting
    print("\n\nüìù SCENARIO 3: Automated Compliance Report Generation")
    print("-" * 70)
    
    report = await system.generate_compliance_report("CUST-12345", days=30)
    print("\n" + report)
    
    # Cleanup
    os.remove(kyc_doc_path)
    system.db.disconnect()
    
    print("\n" + "="*70)
    print(" DEMO COMPLETE")
    print("="*70)


if __name__ == "__main__":
    asyncio.run(main())
